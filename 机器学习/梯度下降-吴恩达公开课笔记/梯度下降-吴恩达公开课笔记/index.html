<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="来自cs的一名渣硕"><title>梯度下降(吴恩达公开课笔记) | 尔了个达</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">梯度下降(吴恩达公开课笔记)</h1><a id="logo" href="/.">尔了个达</a><p class="description">A good brain is not as good as a rotten pen</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/books/"><i class="fa fa-book"> 书籍</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">梯度下降(吴恩达公开课笔记)</h1><div class="post-meta">Sep 3, 2017<span> | </span><span class="category"><a href="/categories/机器学习/">机器学习</a></span></div><div class="post-content"><h1 id="梯度下降思想"><a href="#梯度下降思想" class="headerlink" title="梯度下降思想"></a>梯度下降思想</h1><p>&emsp;&emsp;梯度下降背后的思想是：开始随机选取一个参数的组合$({\theta}_{0},{\theta}_{1},…,{\theta}_{n})$，计算代价函数，然后虚招下一个能让代价函数下降最多的参数组合。一直迭代下去直到找到一个局部最小值停止。因为并没有遍历所有的参数组合，所以不能确定局部最小值就是去全局最小值，如果代价函数具有多个极值点，那么选择不同的初始参数组合，可能会找到不同的局部最小值。<br><a id="more"></a></p>
<h1 id="参数更新"><a href="#参数更新" class="headerlink" title="参数更新"></a>参数更新</h1><p>&emsp;&emsp;当参数只有两个的时候（一个特征）,代价函数为$J({\theta}_{0},{\theta}_{1})$。<br>&emsp;&emsp;repeat until convergence: $${ {\theta}_{j} := {\theta}_{j} - \alpha\frac{<br>\partial}{\partial{\theta}_{j} }J({\theta}_{0},{\theta}_{1}) } \quad for(j=0\quad and\quad  j=1)$$<br>&emsp;&emsp;$temp0:={\theta}_{0}-\alpha\frac{<br>\partial}{\partial{\theta}_{0} }J({\theta}_{0},{\theta}_{1})$<br>&emsp;&emsp;$temp1:={\theta}_{1}-\alpha\frac{<br>\partial}{\partial{\theta}_{1} }J({\theta}_{0},{\theta}_{1})$<br>&emsp;&emsp;${\theta}_{0}:=temp0$<br>&emsp;&emsp;${\theta}_{1}:=temp1$<br>&emsp;&emsp;其中$alpha$是学习率，学习率决定了代价函数下降的大小。在批量梯度下降中，每一次都同时让所有的参数都减去学习速率乘以代价函数的偏导数。</p>
<h1 id="批量梯度下降"><a href="#批量梯度下降" class="headerlink" title="批量梯度下降"></a>批量梯度下降</h1><p>下面的h(x)是要拟合的函数，J(theta)损失函数，theta是参数，要迭代求解的值，theta求解出来了那最终要拟合的函数h(theta)就出来了。其中m是训练集的记录条数，j是参数的个数。<br>（1）将J(theta)对theta求偏导，得到每个theta对应的的梯度<br>（2）由于是要最小化风险函数，所以按每个参数theta的梯度负方向，来更新每个theta<br>（3）从上面公式可以注意到，它得到的是一个全局最优解，但是每迭代一步，都要用到训练集所有的数据，如果m很大，那么可想而知这种方法的迭代速度！！所以，这就引入了另外一种方法，随机梯度下降。</p>
<h1 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h1><p>（1）上面的风险函数可以写成如下这种形式，损失函数对应的是训练集中每个样本的粒度，而上面批量梯度下降对应的是所有的训练样本：<br>（2）每个样本的损失函数，对theta求偏导得到对应梯度，来更新theta<br>（3）随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</p>
<h1 id="两种梯度下降对比"><a href="#两种梯度下降对比" class="headerlink" title="两种梯度下降对比"></a>两种梯度下降对比</h1><h2 id="批量梯度下降-1"><a href="#批量梯度下降-1" class="headerlink" title="批量梯度下降"></a>批量梯度下降</h2><p>&emsp;&emsp;最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小。</p>
<h2 id="随机梯度下降-1"><a href="#随机梯度下降-1" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><p>&emsp;&emsp;最小化每条样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近。</p>
<p><img src="/机器学习/梯度下降-吴恩达公开课笔记/梯度下降-吴恩达公开课笔记/求导.jpg" alt=""></p>
</div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>yitakabe</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/机器学习/梯度下降-吴恩达公开课笔记/梯度下降-吴恩达公开课笔记/">http://www.yingdawei.com/机器学习/梯度下降-吴恩达公开课笔记/梯度下降-吴恩达公开课笔记/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="http://www.yingdawei.com/机器学习/梯度下降-吴恩达公开课笔记/梯度下降-吴恩达公开课笔记/" data-id="cjjtdylcu0015nchoz9p6b20a" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAQ4AAAEOCAAAAABd2qZ5AAAD5UlEQVR42u3awW7bQAwEUP3/T6dATwFSyTPk2kiBp1NQ29LqqcCIXF5XfHz9Pdp///7p89933787/91v765y+MCBAwcOHMGtJqf+eYGcI8d6/s5stcmvcODAgQPHWY7nEJ1htZ/OvpNzRC8KOHDgwIHjgxz5QmclX1J6zV4CcODAgQPHb+bYh27bjiwKMBw4cODA8Ws42i2chKNt2M1KvrZdeKxXigMHDhw4XmfT1YbW//L3G+c7cODAgQPH20Jrv5XVDjfMSs3b9ePAgQMHjjXHbOsoyaZ2rOGZcrMpVX8TBw4cOHAsONqxszxW2+G2WQHZXvEFCg4cOHDgWHPMou65WMo/bUu1d2+V4cCBAweOPcdsdKAttNpWYxKHbdxGDwAHDhw4cKw52oGAZNF5EM62lGa4+WsBDhw4cODYc8wutt/4aafUZsVh2y7EgQMHDhwbjs2gwD4489jLo71tX0ZvHDhw4MCBo1v/dWqhbbbnpV1btu23tXDgwIEDx4xjU54lpVEedfsgn7004MCBAweOsxzvaxTmMXl2eGL/CHHgwIEDx4YjWVahOCr/2mZiPoSRP6qrXSIOHDhw4HisWZKf5SNxm1GDvLjKC8X8tQAHDhw4cOw58uA81f47da1Zc7N+78CBAwcOHOttp83AQVtQtbHaFpZ1bOPAgQMHjo9wzJpueQzvA7Udrbi9Cxw4cODAseZIbmbWImwbdqeCtm1cvth8woEDBw4cJUcydtCOL7RDb3lb8NRqbxFx4MCBA8chjllptNnySUrBWVW1KvBw4MCBA8chjnxZyTf3cXsqmNu+Hw4cOHDg2HPkowZ5mO0beZvm42yrLPpfgAMHDhw4Ao7Nln89iBZf8VQM5yUoDhw4cOA4yzEL1E3RtRmSaIf2ksf2j4oWBw4cOHAsOPL4bPmSCM+Luk3ZWb934MCBAweOQxwJzayIauO5bQ4eiHwcOHDgwHGIY3+rOeIsVjdbVsX6ceDAgQPHGzjagm3W7Dv1MPJPI3ocOHDgwHGUox1Wu8qjjdIcOud+cS84cODAgeMQx+wG2gtvmnf51ZOG4y0uDhw4cOBYc8zCLAm2drjt7FDF5vw4cODAgWNz/q/y2Idx8ZRGow+r8+PAgQMHjjVH29prb6wdhssHFNrxu/wMOHDgwIFjzzEbWWhLrDwINytp//1FcxAHDhw4cCw42nDNmTbNu/bGDrw64MCBAweOD3LsRxD2gwib80eFHA4cOHDg+DjH/sivm8fk/lHhwIEDB46zHHlLLt9kakOxDeD8twUlDhw4cOBYc8xaafmt5iML+YDCZmShbVbiwIEDB46Y4w+2D87hnzl+WwAAAABJRU5ErkJggg==">分享</a><div class="tags"><a href="/tags/机器学习/">机器学习</a><a href="/tags/梯度下降/">梯度下降</a></div><div class="post-nav"><a class="pre" href="/嵌入式/编程/通过函数指针进行初始化/">通过函数指针进行初始化</a><a class="next" href="/数据处理/数据处理算法/时间序列算法/">时间序列算法</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/前端/">前端</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/嵌入式/">嵌入式</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数字信号处理/">数字信号处理</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据分析/">数据分析</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/RK3188/" style="font-size: 15px;">RK3188</a> <a href="/tags/Docker/" style="font-size: 15px;">Docker</a> <a href="/tags/Logistic/" style="font-size: 15px;">Logistic</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/pipenv/" style="font-size: 15px;">pipenv</a> <a href="/tags/三要素/" style="font-size: 15px;">三要素</a> <a href="/tags/前端/" style="font-size: 15px;">前端</a> <a href="/tags/VueJS/" style="font-size: 15px;">VueJS</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/Wifi/" style="font-size: 15px;">Wifi</a> <a href="/tags/蓝牙/" style="font-size: 15px;">蓝牙</a> <a href="/tags/Android5-1/" style="font-size: 15px;">Android5.1</a> <a href="/tags/函数指针/" style="font-size: 15px;">函数指针</a> <a href="/tags/数据分析/" style="font-size: 15px;">数据分析</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/时间序列/" style="font-size: 15px;">时间序列</a> <a href="/tags/梯度下降/" style="font-size: 15px;">梯度下降</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/Python/pipenv的基本使用/">pipenv的基本使用</a></li><li class="post-list-item"><a class="post-list-link" href="/前端/Vue/Vue2学习笔记/">Vue2.0 学习笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/Docker/Docker基本使用/">Docker镜像的基本操作</a></li><li class="post-list-item"><a class="post-list-link" href="/嵌入式/编程/通过函数指针进行初始化/">通过函数指针进行初始化</a></li><li class="post-list-item"><a class="post-list-link" href="/机器学习/梯度下降-吴恩达公开课笔记/梯度下降-吴恩达公开课笔记/">梯度下降(吴恩达公开课笔记)</a></li><li class="post-list-item"><a class="post-list-link" href="/数据处理/数据处理算法/时间序列算法/">时间序列算法</a></li><li class="post-list-item"><a class="post-list-link" href="/机器学习/Logistic函数/">Logistic函数</a></li><li class="post-list-item"><a class="post-list-link" href="/机器学习/机器学习简介/">机器学习简介</a></li><li class="post-list-item"><a class="post-list-link" href="/嵌入式/调试记录/基于RK3188平台与Android5.1的AP6212蓝牙与wifi调试记录/">基于RK3188平台与Android5.1的AP6212蓝牙与wifi调试记录</a></li><li class="post-list-item"><a class="post-list-link" href="/数据处理/数据分析/数据质量分析/">数据质量分析</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">尔了个达.</a><!--|  Powered by--><!--a(rel='nofollow', target='_blank', href='https://hexo.io')  Hexo.--><!--a(rel='nofollow', target='_blank', href='https://github.com/tufu9441/maupassant-hexo')  Theme--><!--|  by--><!--a(rel='nofollow', target='_blank', href='https://github.com/pagecho')  Cho.--></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>